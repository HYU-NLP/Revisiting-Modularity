{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate code properties among 10% of original data and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from datasets import Dataset\n",
    "from utils.utils import get_code_style_score, get_code_modularity_score, read_jsonl_to_dict, write_dict_to_jsonl\n",
    "\n",
    "\n",
    "def compute_code_score(example):\n",
    "    code = example['code']\n",
    "    try:\n",
    "        score_style = get_code_style_score(code)\n",
    "        score_modularity = get_code_modularity_score(code)\n",
    "    except Exception:\n",
    "        score_style = {\n",
    "            'score_var': -1.0,\n",
    "            'score_pep8': -1.0,\n",
    "            'score_style': -1.0,\n",
    "        }\n",
    "        score_modularity = -1.0\n",
    "\n",
    "    example['score_style'] = score_style\n",
    "    example['score_modularity'] = score_modularity\n",
    "    return example\n",
    "\n",
    "\n",
    "def check_code_score(example):\n",
    "    return example['score_style']['score_var'] >= 0 and example['score_style']['score_pep8'] >= 0 and example['score_modularity'] >= 0\n",
    "\n",
    "\n",
    "dataset = read_jsonl_to_dict(os.path.join(os.getcwd(), 'data', 'my_code_contests_train.jsonl'))\n",
    "demonstration = []\n",
    "\n",
    "# aggregate demonstration code\n",
    "# keys for dataset: dict_keys(['name', 'description', 'public_tests', 'private_tests', 'generated_tests', 'source', 'difficulty', 'solutions', 'incorrect_solutions', 'cf_contest_id', 'cf_index', 'cf_points', 'cf_rating', 'cf_tags', 'is_description_translated', 'untranslated_description', 'time_limit', 'memory_limit_bytes', 'input_file', 'output_file'])\n",
    "# keys for solutions: dict_keys(['cc', 'modules', 'passed', 'solution'])\n",
    "for data in dataset:\n",
    "    for i in range(len(data['solutions']['solution'])):\n",
    "        if data['solutions']['passed'][i]:\n",
    "            demonstration.append(\n",
    "                {\n",
    "                    'description': data['description'],\n",
    "                    'code': data['solutions']['solution'][i],\n",
    "                    # more information?\n",
    "                }\n",
    "            )\n",
    "\n",
    "# calculate code metrics\n",
    "random.seed(42)\n",
    "demonstration = random.sample(demonstration, len(demonstration) // 10) # 10% of total data\n",
    "demonstration = Dataset.from_list(demonstration)\n",
    "demonstration = demonstration.map(compute_code_score, num_proc=16)\n",
    "demonstration = demonstration.filter(check_code_score, num_proc=16)\n",
    "\n",
    "# save\n",
    "# demonstration.save_to_disk(os.path.join(os.getcwd(), 'data', 'demonstration'))\n",
    "write_dict_to_jsonl(list(demonstration), os.path.join(os.getcwd(), 'data', 'demonstration.jsonl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get 100 demonstrations of particular code property with evenness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.utils import read_jsonl_to_dict, write_dict_to_jsonl, get_code_style_score, get_code_modularity_score, get_average_length_of_variables\n",
    "\n",
    "\n",
    "random.seed(42) # for reproducibility\n",
    "num_sample = 10 # number of samples to be sampled from each bin\n",
    "\n",
    "# load demonstration pool\n",
    "# each data consists of (problem description, code, style score, modularity score)\n",
    "file_name = 'demonstration'\n",
    "path = f'/home/kdy20401/Workspace/Proj-Code-Generation/MC/data/{file_name}.jsonl'\n",
    "demonstration = read_jsonl_to_dict(path)\n",
    "print(f'number of codes in demonstration pool: {len(demonstration)}')\n",
    "\n",
    "code = []\n",
    "style = [] # score_pep8\n",
    "modularity = [] # score_modularity\n",
    "var_len = []\n",
    "for data in demonstration:\n",
    "    code.append(data['code'])\n",
    "    style.append(data['score_style']['score_pep8'])\n",
    "    modularity.append(data['score_modularity'])\n",
    "    var_len.append(get_average_length_of_variables(data['code']))\n",
    "\n",
    "style_df = pd.DataFrame({'style': np.array(style)})\n",
    "modularity_df = pd.DataFrame({'modularity': np.array(modularity)})\n",
    "var_len_df = pd.DataFrame({'var_len': np.array(var_len)})\n",
    "\n",
    "# bins: 0~0.1, 0.1~0.2, ..., 0.9~1.0\n",
    "num_bin = 10\n",
    "bins = np.linspace(0, 1, num_bin + 1)\n",
    "\n",
    "# find the grid cell to which each data point belongs\n",
    "# include_lowest=True makes 0 style or modularity value included in the first bin\n",
    "# style_df['style_bin'] = pd.cut(style_df['style'], bins=bins, labels=False, include_lowest=True)\n",
    "# modularity_df['modularity_bin'] = pd.cut(modularity_df['modularity'], bins=bins, labels=False, include_lowest=True)\n",
    "var_len_df['var_len_bin'] = pd.cut(var_len_df['var_len'], bins=bins, labels=False, include_lowest=True)\n",
    "\n",
    "# sample data points from each bin\n",
    "# if the number of data points in the bin is less than num_sample, duplication can occur\n",
    "# style_sampled_points = style_df.groupby(['style_bin']).apply(lambda x: x.sample(num_sample, replace=True if len(x) < num_sample else False))\n",
    "# modularity_sampled_points = modularity_df.groupby(['modularity_bin']).apply(lambda x: x.sample(num_sample, replace=True if len(x) < num_sample else False))\n",
    "var_len_sampled_points = var_len_df.groupby(['var_len_bin']).apply(lambda x: x.sample(num_sample, replace=True if len(x) < num_sample else False))\n",
    "\n",
    "# style_sampled_points.index => (style_bin, code_index)\n",
    "# (deduplicated) index of sampled data points \n",
    "# style_index = list(set([e[1] for e in style_sampled_points.index]))\n",
    "# modularity_index = list(set([e[1] for e in modularity_sampled_points.index]))\n",
    "var_len_index = list(set([e[1] for e in var_len_sampled_points.index]))\n",
    "\n",
    "#  the number of samples is less than expected\n",
    "# assert len(style_index) == num_bin * num_sample and len(modularity_index) == num_bin * num_sample\n",
    "assert len(var_len_index) == num_bin * num_sample\n",
    "        \n",
    "selected_demonstration_by_style = [demonstration[i] for i in style_index]\n",
    "selected_demonstration_by_modularity = [demonstration[i] for i in modularity_index]\n",
    "selected_demonstration_by_var_len = [demonstration[i] for i in var_len_index]\n",
    "\n",
    "# save each demonstration which has high coverage of style or modularity\n",
    "# write_dict_to_jsonl(selected_demonstration_by_style, os.path.join(os.getcwd(), 'data', 'style_demonstration.jsonl'))\n",
    "# write_dict_to_jsonl(selected_demonstration_by_modularity, os.path.join(os.getcwd(), 'data', 'modularity_demonstration.jsonl'))\n",
    "write_dict_to_jsonl(selected_demonstration_by_var_len, os.path.join(os.getcwd(), 'data', 'var_len_demonstration.jsonl'))\n",
    "\n",
    "# for visualization\n",
    "# plt.scatter(style_sampled_points['style'], np.array([0.5] * len(style_sampled_points)), color='red', label='Sampled Data')\n",
    "# plt.scatter(modularity_sampled_points['modularity'], np.array([0.5] * len(modularity_sampled_points)), color='blue', label='Sampled Data')\n",
    "# plt.xlabel('Style')\n",
    "# plt.ylabel('Modularity (tmp)')\n",
    "# plt.legend()\n",
    "# plt.show()    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
