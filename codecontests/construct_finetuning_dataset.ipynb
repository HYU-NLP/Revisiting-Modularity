{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide my_code_contests_{split}.jsonl into two by MoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/kdy20401/anaconda3/envs/mc/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 78 75\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gaoya\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from utils.utils import read_jsonl_to_dict, write_dict_to_jsonl\n",
    "\n",
    "\n",
    "split = 'valid'\n",
    "low_mos_range = [0, 0]\n",
    "high_mos_range = [0.7, 1]\n",
    " \n",
    "# make code dataset with low MoS code\n",
    "file = f'/data/kdy20401/Workspace/Proj-Code-Generation/MC/data/ft/my_code_contests_{split}.jsonl'\n",
    "dataset = read_jsonl_to_dict(file)\n",
    "low_dataset = []\n",
    "for pid, data in enumerate(dataset):\n",
    "    # after preprocessing, there can be no correct python solution in the problem\n",
    "    if len(data['solutions']['solution']) == 0:\n",
    "        continue\n",
    "    \n",
    "    code = data['solutions']['solution']\n",
    "    modularity = data['solutions']['modularity']\n",
    "    indicies = []\n",
    "    for i in range(len(code)):\n",
    "        if low_mos_range[0] <= modularity[i] <= low_mos_range[1]:\n",
    "            indicies.append(i)\n",
    "            \n",
    "    dataset[pid]['solutions']['solution'] = [code[i] for i in indicies]\n",
    "    dataset[pid]['solutions']['modularity'] = [modularity[i] for i in indicies]\n",
    "    low_dataset.append(dataset[pid]) # low_dataset = [data1, data2,,,]\n",
    "    \n",
    "\n",
    "# make code dataset with high MoS code\n",
    "file = f'/data/kdy20401/Workspace/Proj-Code-Generation/MC/data/ft/my_code_contests_{split}.jsonl'\n",
    "dataset = read_jsonl_to_dict(file) # should load once again!\n",
    "high_dataset = []\n",
    "for pid, data in enumerate(dataset):\n",
    "    # after preprocessing, there can be no correct python solution in the problem\n",
    "    if len(data['solutions']['solution']) == 0:\n",
    "        continue\n",
    "    \n",
    "    code = data['solutions']['solution']\n",
    "    modularity = data['solutions']['modularity']\n",
    "    indicies = []\n",
    "    for i in range(len(code)):\n",
    "        if high_mos_range[0] <= modularity[i] <= high_mos_range[1]:\n",
    "            indicies.append(i)\n",
    "            \n",
    "    dataset[pid]['solutions']['solution'] = [code[i] for i in indicies]\n",
    "    dataset[pid]['solutions']['modularity'] = [modularity[i] for i in indicies]\n",
    "    high_dataset.append(dataset[pid])\n",
    "    \n",
    "    \n",
    "    \n",
    "# find common problem between the two datasets and save\n",
    "problem1 = set()\n",
    "for data in low_dataset:\n",
    "    if len(data['solutions']['solution']) == 0: # there can be no code in specific MoS range\n",
    "        continue\n",
    "    problem1.add(data['name'])\n",
    "\n",
    "problem2 = set()\n",
    "for data in high_dataset:\n",
    "    if len(data['solutions']['solution']) == 0:\n",
    "        continue\n",
    "    problem2.add(data['name'])\n",
    "\n",
    "\n",
    "common_problem = problem1.intersection(problem2)\n",
    "print(len(problem1), len(problem2), len(common_problem))\n",
    "\n",
    "low_dataset = [data for data in low_dataset if data['name'] in common_problem]\n",
    "high_dataset = [data for data in high_dataset if data['name'] in common_problem]\n",
    "\n",
    "\n",
    "# save\n",
    "write_dict_to_jsonl(low_dataset, os.path.join(os.getcwd(), 'data/ft', f'my_code_contests_{split}_low.jsonl'))\n",
    "write_dict_to_jsonl(high_dataset, os.path.join(os.getcwd(), 'data/ft', f'my_code_contests_{split}_high.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOW MOD DATASET:\n",
      "total number of problem: 75\n",
      "total number of code: 3919\n",
      "average number of codes per problem: 52.25333333333333\n",
      "HIGH MOD DATASET:\n",
      "total number of problem: 75\n",
      "total number of code: 1229\n",
      "average number of codes per problem: 16.386666666666667\n"
     ]
    }
   ],
   "source": [
    "# check data statistics\n",
    "\n",
    "split = 'valid'\n",
    "\n",
    "# low dataset\n",
    "degree = 'low'\n",
    "file = f'/data/kdy20401/Workspace/Proj-Code-Generation/MC/data/ft/my_code_contests_{split}_{degree}.jsonl'\n",
    "dataset = read_jsonl_to_dict(file)\n",
    "print('LOW MOD DATASET:')\n",
    "print(f'total number of problem: {len(dataset)}')\n",
    "\n",
    "num_code = sum([len(data['solutions']['solution']) for data in dataset])\n",
    "print(f'total number of code: {num_code}')\n",
    "print(f'average number of codes per problem: {num_code / len(dataset)}')\n",
    "\n",
    "\n",
    "# high dataset\n",
    "degree = 'high'\n",
    "file = f'/data/kdy20401/Workspace/Proj-Code-Generation/MC/data/ft/my_code_contests_{split}_{degree}.jsonl'\n",
    "dataset = read_jsonl_to_dict(file)\n",
    "print('HIGH MOD DATASET:')\n",
    "print(f'total number of problem: {len(dataset)}')\n",
    "\n",
    "num_code = sum([len(data['solutions']['solution']) for data in dataset])\n",
    "print(f'total number of code: {num_code}')\n",
    "print(f'average number of codes per problem: {num_code / len(dataset)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### deduplicate two dataset (+ flatten) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deduplicate two dataset (+ flatten, same number of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low mod dataset: 61012\n",
      "high mod dataset: 61012\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gaoya\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from utils.utils import read_jsonl_to_dict, write_dict_to_jsonl\n",
    "\n",
    "\n",
    "def deduplicate_codes(codes, max_num_codes=25):\n",
    "    # build index\n",
    "    index = gaoya.minhash.MinHashStringIndex(\n",
    "        hash_size=64, # 64 bit integer for hash value\n",
    "        jaccard_threshold=0.5, # similarity threshold for considering two documents as similar\n",
    "        num_bands=60, # same as paper\n",
    "        band_size=5, # same as paper\n",
    "        num_hashes=60*5, # number of hash values for each document\n",
    "        analyzer='word', # determine how to split the text into tokens (word or char)\n",
    "        lowercase=True, # convert all text to lowercase\n",
    "        ngram_range=(3,4) # use 3,4 grams\n",
    "    )\n",
    "\n",
    "    deduplicated_codes = []\n",
    "    deduplicated_codes_indices = []\n",
    "    indicies = list(range(len(codes)))\n",
    "    random.shuffle(indicies)\n",
    "\n",
    "    # get unique codes\n",
    "    for i in indicies:\n",
    "        code = codes[i]\n",
    "        if len(index.query(code)) == 0:\n",
    "            index.insert_document(i, code)\n",
    "            deduplicated_codes.append(code)\n",
    "            deduplicated_codes_indices.append(i)\n",
    "            \n",
    "        if len(deduplicated_codes) == max_num_codes:\n",
    "            break\n",
    "    \n",
    "    return deduplicated_codes, deduplicated_codes_indices\n",
    "\n",
    "\n",
    "split = 'train'\n",
    "\n",
    "degree = 'low'\n",
    "file = f'/data/kdy20401/Workspace/Proj-Code-Generation/MC/data/ft/my_code_contests_{split}_{degree}.jsonl'\n",
    "low_dataset = read_jsonl_to_dict(file)\n",
    "\n",
    "degree = 'high'\n",
    "file = f'/data/kdy20401/Workspace/Proj-Code-Generation/MC/data/ft/my_code_contests_{split}_{degree}.jsonl'\n",
    "high_dataset = read_jsonl_to_dict(file)\n",
    "\n",
    "\n",
    "low_dedup_dataset, high_dedup_dataset = [], []\n",
    "for idx, (low_data, high_data) in enumerate(zip(low_dataset, high_dataset)):\n",
    "    # code\n",
    "    low_mod_code = low_data['solutions']['solution']\n",
    "    high_mod_code = high_data['solutions']['solution']\n",
    "    \n",
    "    # MoS\n",
    "    low_mod_score = low_data['solutions']['modularity']\n",
    "    high_mod_score = high_data['solutions']['modularity']\n",
    "    \n",
    "    # deduplication per problem (maximum number of codes per problem: 25)\n",
    "    deduplicated_low_mod_code, indicies1 = deduplicate_codes(low_mod_code, 25)\n",
    "    deduplicated_high_mod_code, indicies2 = deduplicate_codes(high_mod_code, 25)\n",
    "    \n",
    "    # sampling same number of codes from each problem\n",
    "    num_min = min(len(deduplicated_low_mod_code), len(deduplicated_high_mod_code))\n",
    "    deduplicated_low_mod_code = random.sample(deduplicated_low_mod_code, num_min)\n",
    "    deduplicated_high_mod_code = random.sample(deduplicated_high_mod_code, num_min)\n",
    "    \n",
    "    # save\n",
    "    low_dataset[idx]['solutions']['solution'] = deduplicated_low_mod_code\n",
    "    low_dataset[idx]['solutions']['modularity'] = [low_mod_score[i] for i in indicies1]\n",
    "    \n",
    "    high_dataset[idx]['solutions']['solution'] = deduplicated_high_mod_code\n",
    "    high_dataset[idx]['solutions']['modularity'] = [high_mod_score[i] for i in indicies2]\n",
    "    \n",
    "    low_dedup_dataset.append(low_dataset[idx])\n",
    "    high_dedup_dataset.append(high_dataset[idx])\n",
    "    \n",
    "    \n",
    "# flatten low_dataset\n",
    "flattened_dataset = [] \n",
    "for data in low_dedup_dataset:\n",
    "    for i in range(len(data['solutions']['solution'])):\n",
    "        flattened_dataset.append({\n",
    "            'name': data['name'],\n",
    "            'description': data['description'],\n",
    "            'public_tests': data['public_tests'],\n",
    "            'private_tests': data['private_tests'],\n",
    "            'source': data['source'],\n",
    "            'difficulty': data['difficulty'],\n",
    "            'cf_contest_id': data['cf_contest_id'],\n",
    "            'cf_index': data['cf_index'],\n",
    "            'cf_points': data['cf_points'],\n",
    "            'cf_rating': data['cf_rating'],\n",
    "            'cf_tags': data['cf_tags'],\n",
    "            'code': data['solutions']['solution'][i],\n",
    "            'modularity': data['solutions']['modularity'][i],\n",
    "        })\n",
    "print(f'low mod dataset: {len(flattened_dataset)}')\n",
    "write_dict_to_jsonl(flattened_dataset, os.path.join(os.getcwd(), 'data/ft', f'my_code_contests_{split}_low_deduplicated.jsonl'))\n",
    "\n",
    "\n",
    "# flatten high_dataset\n",
    "flattened_dataset = [] \n",
    "for data in high_dedup_dataset:\n",
    "    for i in range(len(data['solutions']['solution'])):\n",
    "        flattened_dataset.append({\n",
    "            'name': data['name'],\n",
    "            'description': data['description'],\n",
    "            'public_tests': data['public_tests'],\n",
    "            'private_tests': data['private_tests'],\n",
    "            'source': data['source'],\n",
    "            'difficulty': data['difficulty'],\n",
    "            'cf_contest_id': data['cf_contest_id'],\n",
    "            'cf_index': data['cf_index'],\n",
    "            'cf_points': data['cf_points'],\n",
    "            'cf_rating': data['cf_rating'],\n",
    "            'cf_tags': data['cf_tags'],\n",
    "            'code': data['solutions']['solution'][i],\n",
    "            'modularity': data['solutions']['modularity'][i],\n",
    "        })\n",
    "print(f'high mod dataset: {len(flattened_dataset)}')\n",
    "write_dict_to_jsonl(flattened_dataset, os.path.join(os.getcwd(), 'data/ft', f'my_code_contests_{split}_high_deduplicated.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOW MOD DATASET:\n",
      "total number of problem: 5042\n",
      "total number of code: 61012\n",
      "average number of code per problem: 12.100753669178896\n",
      "HIGH MOD DATASET:\n",
      "total number of problem: 5042\n",
      "total number of code: 61012\n",
      "average number of code per problem: 12.100753669178896\n"
     ]
    }
   ],
   "source": [
    "# Check after deduplication\n",
    "\n",
    "\n",
    "from utils.utils import read_jsonl_to_dict, write_dict_to_jsonl\n",
    "\n",
    "\n",
    "split = 'train'\n",
    "\n",
    "\n",
    "degree = 'low'\n",
    "file = f'/data/kdy20401/Workspace/Proj-Code-Generation/MC/data/ft/my_code_contests_{split}_{degree}_deduplicated.jsonl'\n",
    "dataset = read_jsonl_to_dict(file)\n",
    "\n",
    "print('LOW MOD DATASET:')\n",
    "unique_problem = set()\n",
    "for data in dataset:\n",
    "    unique_problem.add(data['name'])\n",
    "print(f'total number of problem: {len(unique_problem)}')\n",
    "print(f'total number of code: {len(dataset)}')\n",
    "print(f'average number of code per problem: {len(dataset) / len(unique_problem)}')\n",
    "\n",
    "\n",
    "\n",
    "degree = 'high'\n",
    "file = f'/data/kdy20401/Workspace/Proj-Code-Generation/MC/data/ft/my_code_contests_{split}_{degree}_deduplicated.jsonl'\n",
    "dataset = read_jsonl_to_dict(file)\n",
    "\n",
    "print('HIGH MOD DATASET:')\n",
    "unique_problem = set()\n",
    "for data in dataset:\n",
    "    unique_problem.add(data['name'])\n",
    "print(f'total number of problem: {len(unique_problem)}')\n",
    "print(f'total number of code: {len(dataset)}')\n",
    "print(f'average number of code per problem: {len(dataset) / len(unique_problem)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final preprocessing before ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 61012 examples [00:10, 5990.66 examples/s]\n",
      "Generating validation split: 1039 examples [00:00, 12465.73 examples/s]\n",
      "Map: 100%|██████████| 61012/61012 [00:18<00:00, 3308.62 examples/s]\n",
      "Map: 100%|██████████| 1039/1039 [00:00<00:00, 4611.29 examples/s]\n",
      "Creating json from Arrow format: 100%|██████████| 62/62 [00:10<00:00,  5.85ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 18.49ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14942441"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "\n",
    "degree = 'high'\n",
    "\n",
    "train_file = f'data/ft/my_code_contests_train_{degree}_deduplicated.jsonl'\n",
    "validation_file = f'data/ft/my_code_contests_valid_{degree}_deduplicated.jsonl'\n",
    "\n",
    "data_files = {}\n",
    "data_files[\"train\"] = train_file\n",
    "data_files[\"validation\"] = validation_file\n",
    "\n",
    "raw_datasets = load_dataset(\n",
    "    'json',\n",
    "    data_files=data_files,\n",
    "    token=None,\n",
    ")\n",
    "\n",
    "def preprocess_func(example):\n",
    "    instruction = (\n",
    "        \"Write a python code to solve the following coding problem \"\n",
    "        \"that obeys the constraints and passes the example test cases. \"\n",
    "        \"The output code needs to read from and write to standard IO. \"\n",
    "        \"Please wrap your code answer using ```:\"\n",
    "    )\n",
    "\n",
    "    example['text'] = 'Q: ' + instruction + '\\n' + example['description'].strip() + '\\n' + 'A: ```' + example['code'].strip() + '```'\n",
    "    \n",
    "    return example\n",
    "\n",
    "new_datasets = raw_datasets.map(preprocess_func)\n",
    "new_datasets['train'].to_json(os.path.join(os.getcwd(), 'data/ft_final', f'my_code_contests_train_{degree}.jsonl'))\n",
    "new_datasets['validation'].to_json(os.path.join(os.getcwd(), 'data/ft_final', f'my_code_contests_valid_{degree}.jsonl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
